{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "articles = json.load(open('data/articles_processed.json', 'r'))\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make things a bit quicker, let's just work with a subset of articles\n",
    "articles = articles[1000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the title & text of the articles\n",
    "docs = ['\\n'.join([a['title'], a['text']]) for a in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to tell the computer how to break up a text\n",
    "# into meaningful pieces (\"tokens\")\n",
    "# we'll use spacy, a natural language processing library\n",
    "# that will take care of the details for us\n",
    "import spacy\n",
    "from spacy.parts_of_speech import VERB, NUM, NOUN\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we'll define the function that breaks up text into tokens\n",
    "# in particular, the only parts we care about are verbs, numbers,\n",
    "# and 'named entities' (people, places, organizations, etc)\n",
    "def tokenize(doc):\n",
    "    # here we run spacy on the text\n",
    "    # it will identify named entities and tag parts-of-speech\n",
    "    doc = nlp(doc)\n",
    "    ents = [ent.text for ent in doc.ents]\n",
    "    toks = [tok.text for tok in doc\n",
    "            if not tok.is_stop and tok.pos in [VERB, NUM, NOUN]]\n",
    "    return [t.lower() for t in ents + toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to convert this text into some numerical representation\n",
    "# the computer can work with. We'll use \"TF-IDF bag-of-words\".\n",
    "# this process of turning text->numbers is called \"vectorization\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    strip_accents='ascii', # remove accents from characters\n",
    "    lowercase=False,       # don't make things lowercase, this will mess up the NER step\n",
    "    use_idf=True,          # we want to use IDF\n",
    "    smooth_idf=True,       # we want to \"smooth\" the IDF (avoiding division by 0)\n",
    "    max_df=1.0, # ignore terms w/ DF higher than this (int=absolute, float=percent)\n",
    "    min_df=1,   # ignore terms w/ DF lower than this (int=absolute, float=percent)\n",
    "    stop_words='english',  # remove very common English words (e.g. the, a, an)\n",
    "    tokenizer=tokenize     # use our tokenization function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's run the vectorizer on our articles\n",
    "# (this'll take a sec)\n",
    "vecs = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the result is that each article is represented by a list of numbers\n",
    "list(vecs.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so each article is represented by 75,425 numbers, most of which are 0.0\n",
    "# each number corresponds to a token encountered in the dataset\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can get an idea of what's happening using a technique called tSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# `n_components` is the number of dimensions to reduce to\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "# apply the dimensionality reduction\n",
    "# to our embeddings to get our 2d points\n",
    "# this doesn't scale well and the graph gets easily crowded,\n",
    "# so we'll just look at a small amount of articles\n",
    "points = tsne.fit_transform(vecs[:200].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot our results\n",
    "# make it quite big so we can see everything\n",
    "fig, ax = plt.subplots(figsize=(40, 20))\n",
    "\n",
    "# extract x and y values separately\n",
    "xs = points[:,0]\n",
    "ys = points[:,1]\n",
    "\n",
    "# plot the points\n",
    "# we don't actually care about the point markers,\n",
    "# just want to automatically set the bounds of the plot\n",
    "ax.scatter(xs, ys, alpha=1)\n",
    "\n",
    "# annotate each point with its word\n",
    "for i, point in enumerate(points):\n",
    "    ax.annotate(articles[i]['title'],\n",
    "                (xs[i], ys[i]),\n",
    "                fontsize=8)\n",
    "    \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so now we can apply an algorithm to ~cluster~ these articles\n",
    "# i.e. group them so that articles talking about the same/similar things are\n",
    "# in the same group\n",
    "# we'll use a method called DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "model = DBSCAN(\n",
    "    eps=0.2,            # max distance for two points to be considered as the same neighborhood\n",
    "    min_samples=2,      # how many points necessary to define a neighborhood?\n",
    "    metric='cosine',    # how do we define \"distance\"?\n",
    "    algorithm='brute',  # required for cosine metric\n",
    "    n_jobs=-1           # parallelize across all cores\n",
    ")\n",
    "\n",
    "# we can tweak the `eps` value to be more lenient (higher) or stricter (lower)\n",
    "# in how it groups articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = model.fit_predict(vecs)\n",
    "\n",
    "# each article is given a label,\n",
    "# if its above -1, the label is a cluster id\n",
    "# if it is -1, then it's \"noise\"\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we'll take our articles and group them according to these labels\n",
    "# skip those labeled \"noise\" (-1)\n",
    "from collections import defaultdict\n",
    "\n",
    "events = defaultdict(list)\n",
    "for i, clus in enumerate(clusters):\n",
    "    if clus == -1:\n",
    "        continue\n",
    "    events[str(clus)].append(articles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out the clusters\n",
    "for id, mems in events.items():\n",
    "    print('-{}-------------'.format(id))\n",
    "    for a in mems:\n",
    "        print('\\t{}'.format(a['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so we have \"event\" clusteres\n",
    "# now we want to cluster _these clusters_ into \"story\" clusters!\n",
    "# we basically follow a similar process.\n",
    "# we tokenize the whole event (we just mash its articles together)\n",
    "\n",
    "# first we extract the article texts\n",
    "events = list(events.values())\n",
    "docs = ['\\n'.join([a['text'] for a in e]) for e in events]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we vectorize again\n",
    "vecs = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we create another DBSCAN model\n",
    "story_model = DBSCAN(\n",
    "    eps=0.2,\n",
    "    min_samples=2,\n",
    "    metric='cosine',\n",
    "    algorithm='brute',\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and we cluster again!\n",
    "clusters = model.fit_predict(vecs)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# again, we can group them by cluster id\n",
    "# (skipping noise)\n",
    "stories = defaultdict(list)\n",
    "for i, clus in enumerate(clusters):\n",
    "    if clus == -1:\n",
    "        continue\n",
    "    stories[str(clus)].append(events[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll write a little helper function to give us a date and time for an event\n",
    "# we just assume the time the first article in the event was published is when\n",
    "# the event happened. this isn't necessarily true, but it'll be fine for this.\n",
    "def ev_created_at(e):\n",
    "    return min([a['created_at'] for a in e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we can print out the stories\n",
    "from datetime import datetime\n",
    "\n",
    "for id, mems in stories.items():\n",
    "    print('-{}-------------'.format(id))\n",
    "    for e in sorted(mems, key=lambda x: ev_created_at(x)):\n",
    "        created_at = ev_created_at(e)\n",
    "        print('\\t{} : {} ({} articles)'.format(\n",
    "            datetime.fromtimestamp(created_at).strftime('%c'),\n",
    "            e[0]['title'],\n",
    "            len(e)\n",
    "        ))\n",
    "        for a in e:\n",
    "            print('\\t\\t{} ({})'.format(\n",
    "                a['title'],\n",
    "                datetime.fromtimestamp(a['created_at']).strftime('%c'),\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
